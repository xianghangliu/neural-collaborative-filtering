{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianl/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gmf import GMFEngine, GMF\n",
    "from mlp import MLPEngine\n",
    "from neumf import NeuMFEngine\n",
    "from data import SampleGenerator\n",
    "from metrics import MetronAtK\n",
    "\n",
    "import copy\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_hit_ndcg(model, evaluate_data):    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_users, test_items = evaluate_data[0], evaluate_data[1]\n",
    "        negative_users, negative_items = evaluate_data[2], evaluate_data[3]        \n",
    "        test_scores = model(test_users, test_items)\n",
    "        negative_scores = model(negative_users, negative_items)\n",
    "        \n",
    "        metron = MetronAtK(top_k=10)\n",
    "        \n",
    "        metron.subjects = [test_users.data.view(-1).tolist(),\n",
    "                             test_items.data.view(-1).tolist(),\n",
    "                             test_scores.data.view(-1).tolist(),\n",
    "                             negative_users.data.view(-1).tolist(),\n",
    "                             negative_items.data.view(-1).tolist(),\n",
    "                             negative_scores.data.view(-1).tolist()]\n",
    "    hit_ratio, ndcg = metron.cal_hit_ratio(), metron.cal_ndcg()\n",
    "        \n",
    "    \n",
    "    return hit_ratio, ndcg\n",
    "\n",
    "\n",
    "def eval_ce_loss(model, data_loader):\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            users, items, ratings = batch[0], batch[1], batch[2]\n",
    "            ratings = ratings.float()\n",
    "            ratings_pred = model(users, items)\n",
    "            crit = torch.nn.BCELoss()\n",
    "            loss = crit(ratings_pred.view(-1), ratings)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of userId is [0, 942]\n",
      "Range of itemId is [0, 1681]\n"
     ]
    }
   ],
   "source": [
    "ml1m_dir = 'data/ml-100k/u.data'\n",
    "ml1m_rating = pd.read_csv(ml1m_dir, sep='\\t', header=None, names=['uid', 'mid', 'rating', 'timestamp'],  engine='python')\n",
    "# Reindex\n",
    "user_id = ml1m_rating[['uid']].drop_duplicates().reindex()\n",
    "user_id['userId'] = np.arange(len(user_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, user_id, on=['uid'], how='left')\n",
    "item_id = ml1m_rating[['mid']].drop_duplicates()\n",
    "item_id['itemId'] = np.arange(len(item_id))\n",
    "ml1m_rating = pd.merge(ml1m_rating, item_id, on=['mid'], how='left')\n",
    "ml1m_rating = ml1m_rating[['userId', 'itemId', 'rating', 'timestamp']]\n",
    "print('Range of userId is [{}, {}]'.format(ml1m_rating.userId.min(), ml1m_rating.userId.max()))\n",
    "print('Range of itemId is [{}, {}]'.format(ml1m_rating.itemId.min(), ml1m_rating.itemId.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(ml1m_rating.userId.unique())\n",
    "num_items = len(ml1m_rating.itemId.unique())\n",
    "\n",
    "gmf_config = {\n",
    "              'num_users': num_users,\n",
    "              'num_items': num_items,\n",
    "              'latent_dim': 10,\n",
    "              'num_negative': 5,\n",
    "              'l2_regularization': 0, # 0.01\n",
    "              'use_cuda': False,\n",
    "              'device_id': 0,\n",
    "              'model_dir':'checkpoints/{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## federated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = list(idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.dataset[self.idxs[item]]\n",
    "\n",
    "\n",
    "class LocalUpdate:\n",
    "    def __init__(self, model_global, dataset, user_id):\n",
    "        idxs = torch.where(dataset.user_tensor == user_id)[0]\n",
    "        self.local_dataset = DatasetSplit(dataset, idxs)\n",
    "        self.num_local = len(self.local_dataset)        \n",
    "        self.local_model = copy.deepcopy(model_global)\n",
    "        self.user_id = user_id\n",
    "        \n",
    "        \n",
    "    def train(self, lr, epoches_local, local_batch_size):\n",
    "        local_data_loader = DataLoader(self.local_dataset, \n",
    "                                       batch_size=local_batch_size, \n",
    "                                       shuffle=True)\n",
    "        \n",
    "        self.local_model.train()\n",
    "        opt = torch.optim.Adam(self.local_model.parameters(), lr=lr)\n",
    "        for ep_local in range(epoches_local):\n",
    "            t_loss = 0.0\n",
    "            n_batch = 0\n",
    "            for users, items, ratings in local_data_loader:\n",
    "                opt.zero_grad()\n",
    "                ratings = ratings.float()\n",
    "                ratings_pred = self.local_model(users, items).view(-1)\n",
    "                \n",
    "                crit = torch.nn.BCELoss()\n",
    "                loss = crit(ratings_pred, ratings)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                \n",
    "                t_loss += loss.item()\n",
    "                n_batch += 1\n",
    "            \n",
    "            if (ep_local + 1) % 100  == 0:\n",
    "                print(\"user\", self.user_id, \"ep_local\", ep_local, t_loss/n_batch)\n",
    "        \n",
    "        return t_loss/n_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generator = SampleGenerator(ratings=ml1m_rating)\n",
    "\n",
    "model_global = GMF(gmf_config)\n",
    "\n",
    "# keys of user and item embeddings in the model state dict\n",
    "k_u, k_i = 'embedding_user.weight', 'embedding_item.weight'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xianl/code/neural-collaborative-filtering/src/metrics.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x)) # the rank starts from 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 local_loss 0.5712121595266478 total_loss_global 0.5975351829126657 hit_ratio 0.16861081654294804 ndcg 0.07710370916182784\n",
      "1 local_loss 0.5086087770688 total_loss_global 0.5307067714746971 hit_ratio 0.17497348886532343 ndcg 0.07979579597062625\n",
      "2 local_loss 0.4697328703344674 total_loss_global 0.48952186051211133 hit_ratio 0.1823966065747614 ndcg 0.08091705155889964\n",
      "3 local_loss 0.44497335524779214 total_loss_global 0.46671884959618115 hit_ratio 0.18345705196182396 ndcg 0.08165300206167805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6485009ebeb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     train_loader = sample_generator.instance_a_train_loader(num_negatives=gmf_config['num_negative'], \n\u001b[0;32m---> 12\u001b[0;31m                                                             batch_size=1024)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mevaluate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/neural-collaborative-filtering/src/data.py\u001b[0m in \u001b[0;36minstance_a_train_loader\u001b[0;34m(self, num_negatives, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m\"\"\"instance train loader for one training epoch\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtrain_ratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'negative_items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'userId'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negatives'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'negative_items'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_negatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indicator_pre_merge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mjoin_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mldata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    860\u001b[0m             )\n\u001b[1;32m    861\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mleft_indexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_indexer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_join_indexers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;34m\"\"\" return the join indexers \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         return _get_join_indexers(\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_join_indexers\u001b[0;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;31m# `count` is the num. of unique keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;31m# set(lkey) | set(rkey) == range(count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factorize_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;31m# preserve left frame order if how == 'left' and sort == False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_factorize_keys\u001b[0;34m(lk, rk, sort)\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[0mrizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m     \u001b[0mllab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m     \u001b[0mrlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable.pyx\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64Factorizer.factorize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_labels\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"Convert the input to an array.\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def train_fedavg(model_global, sample_generator):\n",
    "frac = 0.1\n",
    "epoches_local = 3\n",
    "local_lr = 1e-2\n",
    "act_samp = False\n",
    "act_agg = False\n",
    "n_clusters = 5\n",
    "\n",
    "hit_ratio_hist, ndcg_hist = [], []\n",
    "for epoch in range(500):\n",
    "    train_loader = sample_generator.instance_a_train_loader(num_negatives=gmf_config['num_negative'], \n",
    "                                                            batch_size=1024)\n",
    "    train_dataset = train_loader.dataset\n",
    "    evaluate_data = sample_generator.evaluate_data\n",
    "\n",
    "    if epoch > 5:\n",
    "        act_samp = True\n",
    "        act_agg = True\n",
    "\n",
    "    # sample users    \n",
    "    if act_samp:\n",
    "        # active sampling\n",
    "        idxs_users = []\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(model_global.state_dict()[k_u].numpy())\n",
    "        for c in range(n_clusters):\n",
    "            c_idxs = np.where(kmeans.labels_ == c)[0]\n",
    "            sel = np.random.choice(c_idxs, max(int(frac * len(c_idxs)), 1))            \n",
    "            idxs_users.append(sel)\n",
    "\n",
    "        idxs_users = np.concatenate(idxs_users)\n",
    "        m = len(idxs_users)\n",
    "        \n",
    "    else:\n",
    "        m = max(int(frac * num_users), 1)\n",
    "        idxs_users = np.random.choice(range(num_users), m, replace=False)\n",
    "\n",
    "    w_locals, n_locals = [], []\n",
    "    total_loss = 0.0\n",
    "    for idx in idxs_users:        \n",
    "        # local update\n",
    "        local_update = LocalUpdate(model_global, train_dataset, user_id=idx)\n",
    "        local_batch_size = int(len(local_update.local_dataset)*0.1)\n",
    "        total_loss += local_update.train(local_lr, epoches_local, local_batch_size=local_batch_size)\n",
    "\n",
    "        w_locals.append(copy.deepcopy(local_update.local_model.state_dict()))\n",
    "        n_locals.append(local_update.num_local)\n",
    "    \n",
    "    # aggregated model\n",
    "    w_avg = {}\n",
    "    \n",
    "    # global model of the previous round, i.e. w0\n",
    "    w_global = model_global.state_dict()\n",
    "    \n",
    "    # aggregated user embeddings\n",
    "\n",
    "    # update delegate embeddings\n",
    "    # line 13 - 16 of Algorithm 3\n",
    "    # user embedding: the k-th chosen user only updates its own user embedding\n",
    "    w_avg[k_u] = copy.deepcopy(w_global[k_u])\n",
    "    for k in range(m):\n",
    "        w_avg[k_u][idxs_users[k]] = w_locals[k][k_u][idxs_users[k]]\n",
    "    \n",
    "    # updating subordinate user embeddings\n",
    "    if act_agg:\n",
    "        # active aggregation\n",
    "        # cluster users\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(w_avg[k_u].numpy())\n",
    "        \n",
    "        # line 19 - 25 of Algorithm 3\n",
    "        # for each cluster c, compute the average user-embedding updates of the delegates in c\n",
    "        # results saved in avg_cluster_update\n",
    "        dim_emb = w_avg[k_u].shape[1]\n",
    "        avg_cluster_update = torch.zeros(n_clusters, dim_emb)\n",
    "        \n",
    "        # number of delegates in each cluster\n",
    "        num_cluster_dels = torch.zeros(n_clusters)\n",
    "        cluster_labels = kmeans.labels_\n",
    "        for k in range(len(idxs_users)):\n",
    "            c = cluster_labels[idxs_users[k]]\n",
    "            avg_cluster_update[c] += w_locals[k][k_u][idxs_users[k]] - w_global[k_u][idxs_users[k]]\n",
    "            num_cluster_dels[c] += 1\n",
    "        \n",
    "        for c in range(n_clusters):\n",
    "            avg_cluster_update[c] /= num_cluster_dels[c]\n",
    "\n",
    "\n",
    "        # line 26 - 30 of Algorithm 3\n",
    "        # apply the cluster average updates to all of the subordinate cluster members\n",
    "        for k in range(num_users):\n",
    "            c = cluster_labels[k]\n",
    "            if k in idxs_users:\n",
    "                continue\n",
    "            w_avg[k_u][k] += avg_cluster_update[c] * np.exp(-epoch)\n",
    "            \n",
    "    # contribution of each user to all items, measured by L1 distance between the \n",
    "    # local item embeddings to the global item embeddings\n",
    "    # line 6 - 9 of Algorithm 3\n",
    "    contrib = torch.zeros(m, num_items)\n",
    "    for k in range(m):\n",
    "        contrib[k] = (w_global[k_i] - w_locals[k][k_i]).abs().sum(1)\n",
    "\n",
    "    eps = 1e-10\n",
    "    contrib = contrib / (contrib.sum(0) + eps)\n",
    "    \n",
    "    # aggregate item embeddings: line 10 - 12 of Algorithm 3\n",
    "    w_avg[k_i] = copy.deepcopy(w_global[k_i])\n",
    "    for i in range(num_items):\n",
    "        if contrib[:, i].sum() == 0:\n",
    "            # item i is not updated by any chosen user, keep the original embedding\n",
    "            continue\n",
    "        \n",
    "        w_avg[k_i][i] *= 0\n",
    "        for k in range(m):\n",
    "            # aggregated embedding of item i: weighted by the contribution of each user\n",
    "            w_avg[k_i][i] += w_locals[k][k_i][i] * contrib[k][i]\n",
    "    \n",
    "    # aggregate other model parameters\n",
    "    # line 2 - 3 of algorithm 3\n",
    "    n_locals = np.array(n_locals, dtype=np.float64)\n",
    "    n_locals /= n_locals.sum()\n",
    "    for k in w_global.keys():\n",
    "        if k != k_u and k != k_i:\n",
    "            w_avg[k] = 0.0\n",
    "            for i in range(len(w_locals)):\n",
    "                w_avg[k] += w_locals[i][k] * n_locals[i]\n",
    "    \n",
    "\n",
    "    model_global.load_state_dict(w_avg)\n",
    "    hit_ratio, ndcg = evaluate_hit_ndcg(model_global, evaluate_data)\n",
    "\n",
    "    total_loss_global = eval_ce_loss(model_global, train_loader)\n",
    "    print(epoch, \"local_loss\", total_loss/m, \"total_loss_global\", total_loss_global, \n",
    "          \"hit_ratio\", hit_ratio, \"ndcg\", ndcg)\n",
    "    \n",
    "    hit_ratio_hist.append(hit_ratio)\n",
    "    ndcg_hist.append(ndcg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hit_ratio_hist)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(ndcg_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
